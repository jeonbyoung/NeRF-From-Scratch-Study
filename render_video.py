import numpy as np
import torch
import imageio
from tqdm import tqdm

from set_device import set_device
from get_samples import get_samples
from volume_rendering import volume_rendering
from Model import Customed_NeRF

# ì¹´ë©”ë¼ê°€ êµ¬(Sphere) í‘œë©´ì„ ë”°ë¼ ëŒë„ë¡ í–‰ë ¬ì„ ë§Œë“œëŠ” í•¨ìˆ˜
def get_spherical_pose(theta, phi, radius):
    trans_t = lambda t : np.array([
        [1,0,0,0], [0,1,0,-0.09], [0,0,1,t], [0,0,0,1],
    ])
    rot_phi = lambda phi : np.array([
        [1,0,0,0], [0,np.cos(phi),-np.sin(phi),0], [0,np.sin(phi), np.cos(phi),0], [0,0,0,1],
    ])
    rot_theta = lambda th : np.array([
        [np.cos(th),0,-np.sin(th),0], [0,1,0,0], [np.sin(th),0, np.cos(th),0], [0,0,0,1],
    ])
    
    # ì¹´ë©”ë¼ ì¢Œí‘œê³„ ë³€í™˜ (NeRFëŠ” -Z ë°©í–¥ì„ ë°”ë¼ë´„)
    c2w = trans_t(radius)
    c2w = rot_phi(phi/180.*np.pi) @ c2w
    c2w = rot_theta(theta/180.*np.pi) @ c2w
    c2w = np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]]) @ c2w
    return c2w[:3,:4]




def render_video(model= None, save_path='result.mp4'):

    print("Video Rendering Started.")

    device = set_device()
    model.eval()

    height = 800
    width = 800

    # np.pi/4.5ëŠ” Blender Dataset(lego, chair, drum) ë“±ì— ëŒ€í•´ì„œ ê³µí†µì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê°’ì´ì§€ë§Œ,
    # ë‚˜ì¤‘ì— ì§ì ‘ ì°ì€ ê²Œ ìˆë‹¤ë©´, ê·¸ì— ë§ëŠ” ë Œì¦ˆê°?ì„ ì¤˜ì•¼í•œë‹¤.
    focal_length = 800/2/ np.tan(np.pi/4.5)

    render_poses = [get_spherical_pose(angle, -30.0, 4.0) for angle in np.linspace(-180, 180, 40+1)[:-1]] 

    # ì§„ì§œ ìµœì¢…ìœ¼ë¡œ ì´ë¯¸ì§€ ë”´ ê±¸ ë„£ì„ ê³³ì´ë‹¤.
    frames = []


    for c2w in tqdm(render_poses):
        # 1 ray ìƒì„±
        # ìœ„ì—ì„œ ì„ì˜ë¡œ êµ¬í•œ ì € ì›í˜•ìœ¼ë¡œ ë„ëŠ” ê¶¤ì ì˜ ìœ„ì¹˜ë¥¼ ë”°ëŠ” get_spherical_poseë¡œ,
        # ì´ì „ì— Rotì— í•´ë‹¹í•˜ëŠ” ê²ƒì„ ì´ë²ˆì—” c2w(camera to world)ë¡œ êµ¬í•´ë³´ì.
        pixel_x, pixel_y = np.meshgrid(np.arange(width, dtype=np.float32),
                                       np.arange(height, dtype=np.float32),
                                       indexing='xy')
    
        dirs = np.stack([(pixel_x - width/2)/focal_length, -(pixel_y - width/2)/focal_length, -np.ones_like(pixel_x)],axis= -1)

        rays_d = dirs @ c2w[:3, :3].T
        rays_o = np.broadcast_to(c2w[:3, -1], rays_d.shape)

        # 2 slicing and rendering
        # 800 by 800 ì§œë¦¬ë¥¼ í•œ ë²ˆì— ëª¨ë¸ì— ë„£ì—ˆë‹¤ê°€ëŠ” ë¶€ë‹´ì´ í¬ë‹¤ê³  í•œë‹¤.
        # slicingí•´ì„œ ë„£ì–´ì£¼ê³ , frames_pixelsì—ì„œ ëª¨ì•„ë†“ê³  ê´€ë¦¬í•˜ì.
        # ê·¸ê±° ë§ê³ ëŠ” ê·¸ëƒ¥ ê·¸ë™ì•ˆì— í–ˆë˜ ê±°ë‘ ìœ ì‚¬í•˜ë‹¤.
        rays_o_flat = torch.from_numpy(rays_o).float().reshape(-1,3).to(device)
        rays_d_flat = torch.from_numpy(rays_d).float().reshape(-1,3).to(device)

        frame_pixels = []
        chunk_size = 1024

        for i in range(0,rays_o_flat.shape[0], chunk_size):
            batch_o = rays_o_flat[i:i+chunk_size]
            batch_d = rays_d_flat[i:i+chunk_size]

            pts, t_values = get_samples(batch_d, batch_o, num_of_samples=64)

            pts_flat = pts.reshape(-1,3)

            dirs_expanded = batch_d[:,None,:].expand_as(pts)
            dirs_flat = dirs_expanded.reshape(-1,3)

            raw_rgb, raw_sigma = model.forward(pts_flat, dirs_flat)


            rgb_for_vr = raw_rgb.reshape(batch_o.shape[0],64,3)
            sigma_for_vr = raw_sigma.reshape(batch_o.shape[0],64)

            rgb_chunk = volume_rendering(rgb_for_vr, sigma_for_vr, t_values)

            # ì¤‘ê°„ì— ê°‘ìê¸° ì™œ cpuë¡œ ëŒë¦¬ëŠëƒ
            # ë©”ëª¨ë¦¬ ì ˆì•½ ìœ„í•¨ì´ë¼ê³  í•œë‹¤. gpuì— ëª¨ë“  ì •ë³´ë¥¼ ë‹¤ ì˜¬ë ¸ë‹¤ê°€ í„°ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ, ì´ëŸ° ì²˜ë¦¬ë¥¼ í•œë‹¤ê³  í•œë‹¤.
            frame_pixels.append(rgb_chunk.cpu())

        # 3 Synthesize
        # ì´ì œ frame_pixelsì— ëª¨ì•„ë’€ë˜ ê²ƒì„ í•©ì¹  ì‹œê°„ì´ë‹¤.
        final_img_flat = torch.cat(frame_pixels, dim=0)
        final_img = final_img_flat.reshape(height, width, 3)

        # ì‹¤ì œ rgbëŠ” 0~1ì˜ ì‹¤ìˆ˜ê°€ ì•„ë‹Œ, 0~255ì˜ ì •ìˆ˜ë¡œ êµ¬ì„±ëœë‹¤.
        final_img = (np.clip(final_img.numpy(), 0, 1)* 255).astype(np.uint8)

        frames.append(final_img)

    print(f"Saving Video to {save_path}, just wait for the set up ended...")
    imageio.mimwrite(save_path, frames, fps=30, quality=8)
    print("ğŸˆğŸ‰ğŸ¥³ Done! ğŸˆğŸ‰ğŸ¥³")

    model.train()